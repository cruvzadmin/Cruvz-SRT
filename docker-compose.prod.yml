# ===============================================================================
# CRUVZ STREAMING - SIMPLIFIED PRODUCTION DOCKER COMPOSE
# Optimized for zero-error deployment and live production use
# ===============================================================================

services:
  # ================================
  # BACKEND API SERVER
  # ================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        - NODE_ENV=production
    container_name: cruvz-backend-prod
    restart: unless-stopped
    ports:
      - "5000:5000"
    volumes:
      - ./data/database:/app/data
      - ./data/logs/backend:/app/logs
      - ./data/uploads:/app/uploads
      - ./.env.production:/app/.env:ro
    environment:
      - NODE_ENV=production
      - PORT=5000
      - DATABASE_URL=/app/data/cruvz_production.db
      - LOG_LEVEL=info
      - ENABLE_CORS=true
      - ENABLE_HELMET=true
      - ENABLE_RATE_LIMITING=true
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    # CORRECTION: ensure backend waits for origin (origin provides streaming endpoints)
    depends_on:
      origin:
        condition: service_healthy

  # ================================
  # STREAMING ENGINE (ORIGIN)
  # ================================
  origin:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILD_MODE=production
    container_name: cruvz-origin-prod
    restart: unless-stopped
    ports:
      - "8080:8080"      # Origin server / API
      - "1935:1935"      # RTMP
      - "9999:9999/udp"  # SRT
      - "3333:3333"      # WebRTC signaling
      - "3334:3334"      # WebRTC TLS signaling
      - "10000-10010:10000-10010/udp"  # WebRTC candidates
    volumes:
      - ./configs:/opt/ovenmediaengine/bin/origin_conf:ro
      - ./data/recordings:/opt/ovenmediaengine/recordings
      - ./data/logs/origin:/opt/ovenmediaengine/logs
      - ./ssl:/opt/ovenmediaengine/ssl:ro
    environment:
      - OME_HOST_IP=0.0.0.0
      - OME_RTMP_PROV_PORT=1935
      - OME_SRT_PROV_PORT=9999
      - OME_WEBRTC_SIGNALLING_PORT=3333
      - OME_WEBRTC_SIGNALLING_TLS_PORT=3334
      - OME_API_PORT=8080
      - LOG_LEVEL=INFO
      # CORRECTION: added candidate port range to match exposed UDP ports
      - OME_WEBRTC_CANDIDATE_PORT=10000-10010
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 4              # CORRECTION: slight increase for slower startup
      start_period: 70s       # CORRECTION: extra buffer
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'

  # ================================
  # WEB APPLICATION
  # ================================
  web-app:
    image: nginx:alpine
    container_name: cruvz-webapp-prod
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./web-app:/usr/share/nginx/html:ro
      - ./web-app/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./data/logs/nginx:/var/log/nginx
      - ./ssl:/etc/nginx/ssl:ro
    environment:
      - BACKEND_API_URL=http://backend:5000
      - ENVIRONMENT=production
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'

  # ================================
  # MONITORING - PROMETHEUS
  # ================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: cruvz-prometheus-prod
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.max-concurrency=20'
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # ================================
  # MONITORING - GRAFANA
  # ================================
  grafana:
    image: grafana/grafana-oss:10.2.2
    container_name: cruvz-grafana-prod
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana-dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana-datasources:/etc/grafana/provisioning/datasources:ro
      - ./data/logs/grafana:/var/log/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-cruvz123}   # CORRECTION: parameterized
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_LOG_LEVEL=warn
      - GF_PATHS_LOGS=/var/log/grafana
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # ================================
  # REDIS CACHE
  # ================================
  redis:
    image: redis:7.2-alpine
    container_name: cruvz-redis-prod
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./monitoring/redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - cruvz-prod-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'

# ================================
# NETWORKS
# ================================
networks:
  cruvz-prod-network:
    name: cruvz-production-optimized
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1

# ================================
# VOLUMES
# ================================
volumes:
  prometheus-data:
    name: cruvz-prometheus-prod-data
    driver: local
  grafana-data:
    name: cruvz-grafana-prod-data
    driver: local
  redis-data:
    name: cruvz-redis-prod-data
    driver: local
